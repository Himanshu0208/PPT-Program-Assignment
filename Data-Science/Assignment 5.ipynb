{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52eaf112",
   "metadata": {},
   "source": [
    "# Naive Approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120516ae",
   "metadata": {},
   "source": [
    "**Ques 1. What is the Naive Approach in machine learning?**\n",
    "\n",
    "In machine learning the naive apporach is also known as the Naive Bayes Classifier, is a simple probabilistic classification algorithm based on baye's theorem. It is very simple and even then is widely used for some of the real world applications. The common feild of application of it is text classification, email spam detection and also sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd743fd",
   "metadata": {},
   "source": [
    "**Ques2. Explain the assumptions of feature independence in the Naive Approach.**\n",
    "\n",
    "the main assumption of the approach is that. all the features are conditionally independent.\n",
    "due to this simple assumption we can just write the joint probability of the features as the product of the individual probabilites of each feature\n",
    "\n",
    "P(X₁, X₂, ..., Xₙ | Y) ≈ P(X₁ | Y) * P(X₂ | Y) * ... * P(Xₙ | Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612ffa8c",
   "metadata": {},
   "source": [
    "**Ques3. How does the Naive Approach handle missing values in the data?**\n",
    "\n",
    "the naive approach handels missing values in the data by ignoring the instances with missing values during the probability estimation process.  It focuses on the available features and assumes that missing values do not contribute to the classification decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528b5530",
   "metadata": {},
   "source": [
    "**Ques4. What are the advantages and disadvantages of the Naive Approach?**\n",
    "\n",
    "__Advantages:__\n",
    "- it is very simple \n",
    "- it is very fast\n",
    "- it handels the missing values in the data\n",
    "- it is computanioally very effiient\n",
    "\n",
    "__Disadvantage:__\n",
    "- It is strongly based on the assumption that the features as conditionally independent which is not true in all real world problems \n",
    "- It may face the \"zero-frequency problem\" when encountering words or feature values that were not present in the training data. This can cause probabilities to be zero, leading to incorrect predictions.\n",
    "- It is only designed for the classification problem and can't able to solve regression problem esaily\n",
    "- In case of rare events it may lead to unreliable probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0210840a",
   "metadata": {},
   "source": [
    "**Ques5. Can the Naive Approach be used for regression problems? If yes, how?**\n",
    "\n",
    "No the naive approach can't be used for the regression problem because it works on finding the probablity of new point for all classes and then select the one with the highest prob.\n",
    "\n",
    "The Naive Approach works based on the assumption of feature independence given the class label, which allows for the calculation of conditional probabilities.\n",
    "\n",
    "In regression problems, the goal is to predict a continuous target variable based on the input features. The Naive Approach, which is based on probabilistic classification, does not have a direct mechanism to handle continuous target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c976f366",
   "metadata": {},
   "source": [
    "**Ques6. How do you handle categorical features in the Naive Approach?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a33730",
   "metadata": {},
   "source": [
    "Their are various ways to handel categorical variables in the naive approach :-\n",
    "\n",
    "_**Label Encoding**_\n",
    "- In this we give different ranks to each of the category in that feature. it requires domain knowledge for selcting the ranks.\n",
    "- ex = if if have a feature containg category like 'Human', 'cat', 'dog', now according to the label encoding we give 1 to 'Human' , 2 to 'cat' and 3 'dog'\n",
    "- However, this method introduces an arbitrary order to the categories, which may not be appropriate for some features where the order doesn't have any significance.\n",
    "\n",
    "-**One hot encoding**_\n",
    "- In this we make dummy features of all category containg either 0 or 1 on one of these dummy class will be having 1 and others will be having only 0. the dummy feature with 1 will be the class for that data point \n",
    "- ex = if have a feature 'Type' containg category like 'Human', 'cat', 'dog', now according to the one hot encoding we make dummy features as 'type_Human' , 'type_cat' , 'type_dog' and if for a row the type was 'cat' than for that row value of these mentioned dummy classes will be [0,1,0]\n",
    "- One-hot encoding avoids the issue of introducing arbitrary order but can result in a high-dimensional feature space, especially when dealing with a large number of categories.\n",
    "\n",
    "_**Binary Class Encoding**_\n",
    "- in this we use binary represntaion for showing the category.\n",
    "- ex => if have a feature 'Type' containg classes like 'Human', 'cat', 'dog', now according to the Binary Class encoding we make 'Human' as 00 , 'cat' as 01 , 'dog' as 10\n",
    "- the dimensionality compared to one-hot encoding while preserving some information about the categories.\n",
    "\n",
    "\n",
    "_**Count Encoding**_\n",
    "   - Count encoding replaces each category with the count of its occurrences in the dataset.\n",
    " - For example, if we have a feature \"type\" with categories \"Human\", \"cat\" and \"dog\" count encoding would replace them with the respective counts of instances belonging to each type.\n",
    "- This method captures the frequency information of each category and can be useful when the count of occurrences is informative for the classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156dd2b8",
   "metadata": {},
   "source": [
    "**Ques7. What is Laplace smoothing and why is it used in the Naive Approach?**\n",
    "\n",
    "Laplace smoothing, also known as add-one smoothing or additive smoothing, is a technique used in the Naive Approach (Naive Bayes classifier) to address the issue of zero probabilities for unseen categories or features in the training data. It is used to prevent the probabilities from becoming zero and to ensure a more robust estimation of probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389cf62",
   "metadata": {},
   "source": [
    "**Ques8. How do you choose the appropriate probability threshold in the Naive Approach?**\n",
    "\n",
    "Here are some considerations to help you choose an appropriate probability threshold:\n",
    "\n",
    "- _**Evaluate the cost of misclassification:**_ Consider the consequences of misclassifying a document. Are false positives (misclassifying a document as spam when it's not) more costly, or are false negatives (misclassifying a spam document as not spam) more problematic? The threshold can be adjusted to prioritize minimizing one type of error over the other.\n",
    "- _**Analyze the class distribution:**_ Examine the proportion of documents in each class in the training data. If the classes are imbalanced, meaning one class has significantly more instances than the other, you may need to adjust the threshold to account for the skewed distribution. For example, if the spam class is rare, you might want to lower the threshold to capture more potential spam emails.\n",
    "- _**Consider the desired precision and recall trade-off:**_ Precision measures the proportion of correctly classified positive instances (spam) out of all instances classified as positive. Recall measures the proportion of correctly classified positive instances out of all actual positive instances. Adjusting the threshold can impact the precision and recall trade-off. A higher threshold may result in higher precision but lower recall, while a lower threshold may increase recall but decrease precision.\n",
    "- _**Use domain knowledge:**_ Consider any domain-specific knowledge or requirements that may influence the choice of threshold. For example, if you have specific guidelines or regulations regarding spam detection, they may guide you in setting a threshold that aligns with those requirements.\n",
    "- _**Validate and fine-tune:**_ It is essential to evaluate the performance of the classifier using appropriate evaluation metrics such as accuracy, precision, recall, or F1 score. You can use techniques like cross-validation or hold-out validation to assess the classifier's performance with different threshold values. Fine-tune the threshold based on the desired performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ae6e5",
   "metadata": {},
   "source": [
    "**Ques9. Give an example scenario where the Naive Approach can be applied.**\n",
    "\n",
    "1. _**Dataset:**_ Collect a dataset of labeled emails, where each email is labeled as either \"spam\" or \"not spam\". The dataset should include a variety of emails with different characteristics and content.\n",
    "2. _**Preprocessing:**_ Preprocess the text of the emails by removing stopwords, punctuation, and converting the words to lowercase. This step helps to normalize the text and remove irrelevant information.\n",
    "3. _**Feature Extraction:**_ Create a vocabulary of all unique words in the training data. For each email, represent it as a feature vector that indicates the presence or absence of each word in the vocabulary. This step converts the text data into a numerical representation that can be used by the Naive Bayes algorithm.\n",
    "4. _**Training:**_ Use the labeled dataset to train the Naive Bayes classifier. Calculate the likelihood probabilities of each word appearing in each class (spam or not spam) by counting the occurrences of each word in the respective class and dividing it by the total number of words in that class.\n",
    "5. _**Prior Probabilities:**_ Calculate the prior probabilities of each class based on the proportion of the training data belonging to each class. This step determines the initial probability of encountering each class before considering the specific words in the email.\n",
    "6. _**Classification:**_ Given a new, unseen email, calculate the posterior probability of each class using Bayes' theorem. Multiply the prior probability of the class with the likelihood probabilities of each word appearing in that class. Finally, select the class with the highest posterior probability as the predicted class for the new email.\n",
    "7. _**Evaluation:**_ Evaluate the performance of the Naive Bayes classifier using appropriate evaluation metrics such as accuracy, precision, recall, or F1 score. This step helps assess the effectiveness of the classifier in correctly classifying spam and non-spam emails.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74039202",
   "metadata": {},
   "source": [
    "# KNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce0a12",
   "metadata": {},
   "source": [
    "**Ques10. What is the K-Nearest Neighbors (KNN) algorithm?**\n",
    "\n",
    "the K-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm used for both classification and regression task. It is a non-parametric algorithm. it predicts the value based on the simalrity with its k-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02f22fb",
   "metadata": {},
   "source": [
    "**Ques11. How does the KNN algorithm work?**\n",
    "\n",
    "1. Training phase :\n",
    "    - During this phase the algorithm stores the whole data\n",
    "<br><br>\n",
    "2. Prediction Phase :\n",
    "    - when a new data point comes it calulates it's similarity with every data point given during training phase (the similarity is calculated by calculation the distance of that point from other point). and then selects the k nearest neighbors\n",
    "<br><br>\n",
    "3. Decision Phase :\n",
    "    - classification : It will select the most reccuring class as the value for the data point\n",
    "    - regression : it will take the avg of all the values of k-neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c742d27",
   "metadata": {},
   "source": [
    "**Ques12. How do you choose the value of K in KNN?**\n",
    "\n",
    "1. Rule of thumb: acc to this rule the value of k should be the sqrt of the total number of instance. it provides a balanced trade off between capturing the local patterns(small k) and incorporating global information (large K) \n",
    "\n",
    "2. Cross-validation : it is a robust technique for evaluating the performance of a model on data. you can perform k-fold cross-validation, where we split data into K equally sized folds and iterate over diffrenct values of k. then the one with best performance will be selected\n",
    "\n",
    "3. odd vs even k: for a binary classification we should use odd value ok k so that there is no condition where tie can occur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488c3075",
   "metadata": {},
   "source": [
    "**Ques13. What are the advantages and disadvantages of the KNN algorithm?**\n",
    "\n",
    "1. advantages:\n",
    "    - easy to understand and implement\n",
    "    - it does not require a training phase. hence its flexible and addaptive\n",
    "    - it can capture complex decion boundry including non linear ones\n",
    "    - it is robust to outliers. as they are less in number hence if some are in the k nearest neighbors even then they have less influence\n",
    "<br><br>\n",
    "2. disadvantage :\n",
    "    - with large data set its computationally very expensive. because it reques calculating distance with each point\n",
    "     - it is sensitive to featue scaling. feature with larger scales can dominate the distance calculation, leading to biased result. hence feature scaling such as standardization is often necessary\n",
    "     - KNN suffers from the curse of dimensionality, where the performance degrades as the number of features increases. As the feature space becomes more sparse in higher dimensions, the distance-based similarity measure becomes less reliable.\n",
    "     - It may struggle with imbalanced datasets where one class dominates the others.\n",
    "     - the perfect choice of k is subjective and problem dependent. a small value of k may lead to overfitting, while a large value of k may lead to underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d86d0c",
   "metadata": {},
   "source": [
    "**Ques14. How does the choice of distance metric affect the performance of KNN?**\n",
    "\n",
    "\n",
    "1. Euclidean Distance:\n",
    "   - Euclidean distance is the most commonly used distance metric in KNN. It calculates the straight-line distance between two instances in the feature space.\n",
    "   - Euclidean distance works well when the feature scales are similar and there are no specific considerations regarding the relationships between features.\n",
    "   - However, it can be sensitive to outliers and the curse of dimensionality, especially when dealing with high-dimensional data.\n",
    "<br><br>\n",
    "2. Manhattan Distance:\n",
    "   - Manhattan distance, also known as city block distance or L1 norm, calculates the sum of absolute differences between corresponding feature values of two instances.\n",
    "   - Manhattan distance is more robust to outliers compared to Euclidean distance and is suitable when the feature scales are different or when there are distinct feature dependencies.\n",
    "   - It performs well in situations where the directions of feature differences are more important than their magnitudes.\n",
    "<br><br>\n",
    "3. Minkowski Distance:\n",
    "   - Minkowski distance is a generalized form that includes both Euclidean distance and Manhattan distance as special cases.\n",
    "   - It takes an additional parameter, p, which determines the degree of the distance metric. When p=1, it is equivalent to Manhattan distance, and when p=2, it is equivalent to Euclidean distance.\n",
    "   - By varying the value of p, you can control the emphasis on different aspects of the feature differences.\n",
    "<br><br> \n",
    "4. Cosine Similarity:\n",
    "   - Cosine similarity measures the cosine of the angle between two vectors. It calculates the similarity based on the direction rather than the magnitude of the feature vectors.\n",
    "   - Cosine similarity is widely used when dealing with text data or high-dimensional sparse data, where the magnitude of feature differences is less relevant.\n",
    "   - It is especially useful when the absolute values of feature magnitudes are not important, and the focus is on the relative orientations or patterns between instances.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1a36b4",
   "metadata": {},
   "source": [
    "**Ques15. Can KNN handle imbalanced datasets? If yes, how?**\n",
    "\n",
    "1. Adjusting Class Weights:\n",
    "   - One way to handle imbalanced datasets is by adjusting the weights of the classes during the prediction phase.\n",
    "   - By assigning higher weights to minority classes and lower weights to majority classes, the algorithm can give more importance to the instances from the minority class during the nearest neighbor selection process.\n",
    "<br><br>\n",
    "2. Oversampling:\n",
    "   - Oversampling techniques involve creating synthetic instances for the minority class to balance the dataset.\n",
    "   - One popular oversampling method is the Synthetic Minority Over-sampling Technique (SMOTE), which generates synthetic instances by interpolating feature values between nearest neighbors of the minority class.\n",
    "   - Oversampling helps in increasing the representation of the minority class, providing a more balanced dataset for KNN to learn from.\n",
    "<br><br>\n",
    "3. Undersampling:\n",
    "   - Undersampling techniques involve randomly selecting a subset of instances from the majority class to balance the dataset.\n",
    "   - By reducing the number of instances in the majority class, undersampling can help prevent the algorithm from being biased towards the majority class during prediction.\n",
    "   - However, undersampling may result in loss of important information and can be more prone to overfitting if the available instances are limited.\n",
    "<br><br>\n",
    "4. Ensemble Approaches:\n",
    "   - Ensemble methods like Bagging or Boosting can be used to address the imbalanced dataset issue.\n",
    "   - Bagging involves creating multiple subsets of the imbalanced dataset, balancing each subset, and training multiple KNN models on these subsets. The final prediction is made by aggregating the predictions of all models.\n",
    "   - Boosting techniques like AdaBoost or Gradient Boosting give more weight to instances from the minority class during training, enabling the model to focus on correctly classifying minority instances.\n",
    "<br><br>\n",
    "5. Evaluation Metrics:\n",
    "   - When dealing with imbalanced datasets, accuracy alone may not provide an accurate assessment of model performance.\n",
    "   - It is important to consider other evaluation metrics such as precision, recall, F1-score, or area under the ROC curve (AUC-ROC) that provide insights into the model's ability to correctly classify instances from the minority class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ddada0",
   "metadata": {},
   "source": [
    "**Ques16. How do you handle categorical features in KNN?**\n",
    "\n",
    "Their are various ways to handel categorical variables in the naive approach :-\n",
    "\n",
    "_**Label Encoding**_\n",
    "- In this we give different ranks to each of the category in that feature. it requires domain knowledge for selcting the ranks.\n",
    "- ex = if if have a feature containg category like 'Human', 'cat', 'dog', now according to the label encoding we give 1 to 'Human' , 2 to 'cat' and 3 'dog'\n",
    "- However, this method introduces an arbitrary order to the categories, which may not be appropriate for some features where the order doesn't have any significance.\n",
    "\n",
    "-**One hot encoding**_\n",
    "- In this we make dummy features of all category containg either 0 or 1 on one of these dummy class will be having 1 and others will be having only 0. the dummy feature with 1 will be the class for that data point \n",
    "- ex = if have a feature 'Type' containg category like 'Human', 'cat', 'dog', now according to the one hot encoding we make dummy features as 'type_Human' , 'type_cat' , 'type_dog' and if for a row the type was 'cat' than for that row value of these mentioned dummy classes will be [0,1,0]\n",
    "- One-hot encoding avoids the issue of introducing arbitrary order but can result in a high-dimensional feature space, especially when dealing with a large number of categories.\n",
    "\n",
    "_**Binary Class Encoding**_\n",
    "- in this we use binary represntaion for showing the category.\n",
    "- ex => if have a feature 'Type' containg classes like 'Human', 'cat', 'dog', now according to the Binary Class encoding we make 'Human' as 00 , 'cat' as 01 , 'dog' as 10\n",
    "- the dimensionality compared to one-hot encoding while preserving some information about the categories.\n",
    "\n",
    "\n",
    "_**Count Encoding**_\n",
    "   - Count encoding replaces each category with the count of its occurrences in the dataset.\n",
    " - For example, if we have a feature \"type\" with categories \"Human\", \"cat\" and \"dog\" count encoding would replace them with the respective counts of instances belonging to each type.\n",
    "- This method captures the frequency information of each category and can be useful when the count of occurrences is informative for the classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f42df89",
   "metadata": {},
   "source": [
    "**Ques17. What are some techniques for improving the efficiency of KNN?**\n",
    "\n",
    "1. Normalize the training data: Since KNN relies on distance for classification, normalizing the training data can improve its accuracy dramatically. If the features represent different physical units or come in vastly different scales, normalizing the data can help ensure that no single feature dominates the distance calculation.\n",
    "2. Assign weights to neighbors: Assigning weights to the contributions of the neighbors can be a useful technique to improve the accuracy of KNN. By giving each neighbor a weight based on its distance to the query point (e.g., weight = 1/distance), you can make the nearer neighbors contribute more to the average than the more distant ones. This can be implemented by modifying the distance calculation or using weighted voting during classification.\n",
    "3. Choose an appropriate value for k: The choice of the value of k in KNN can significantly impact its performance. Larger values of k reduce the effect of noise on the classification but may make the boundaries between classes less distinct. Selecting an optimal value for k can be done using various heuristic techniques or hyperparameter optimization methods. It is also helpful to choose an odd value for k in binary classification problems to avoid tied vote\n",
    "4. Consider dimensionality reduction: When dealing with high-dimensional data, dimensionality reduction techniques like Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), or Canonical Correlation Analysis (CCA) can be applied as a pre-processing step before applying KNN. Dimensionality reduction helps to avoid the curse of dimensionality, where Euclidean distance becomes less meaningful in high-dimensional spaces. By reducing the dimensionality of the data, you can improve the efficiency and accuracy of KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ec33e",
   "metadata": {},
   "source": [
    "**Ques18. Give an example scenario where KNN can be applied.**\n",
    "\n",
    "__Scenario:__ Customer Segmentation for a Retail Store\n",
    "\n",
    "A retail store wants to segment its customers based on their purchasing behavior to better understand their preferences and target them with personalized marketing campaigns. The store has collected data on customer attributes such as age, income, and purchase history.\n",
    "\n",
    "In this scenario, KNN can be applied as follows:\n",
    "\n",
    "1. __Data Preparation:__ The retail store prepares the dataset by cleaning and preprocessing the customer data. This may involve handling missing values, encoding categorical variables, and normalizing numerical features.\n",
    "2. __Feature Selection:__ The store selects relevant features that are likely to influence customer segmentation, such as age, income, and purchase frequency. Other features like gender, location, or product preferences can also be considered.\n",
    "3. __Training and Testing Data:__ The dataset is split into training and testing sets. The training set is used to build the KNN model, while the testing set is used to evaluate its performance.\n",
    "4. __Model Training:__ The KNN model is trained using the training data. The model learns the patterns and relationships between the customer attributes and their corresponding segments.\n",
    "5. __Hyperparameter Tuning:__ The value of K, the number of nearest neighbors to consider, is an important hyperparameter in KNN. The retail store can use techniques like cross-validation or grid search to find the optimal value of K that maximizes the model's performance.\n",
    "6. __Model Evaluation:__ The trained KNN model is evaluated using the testing data. Performance metrics such as accuracy, precision, recall, or F1-score can be calculated to assess how well the model predicts the customer segments.\n",
    "7. __Customer Segmentation:__ Once the KNN model is trained and evaluated, it can be used to segment new customers based on their attributes. Given the attributes of a new customer, the model predicts the segment to which the customer belongs by considering the K nearest neighbors in the training data.\n",
    "8. __Marketing Campaigns:__ The retail store can use the customer segments identified by the KNN model to tailor marketing campaigns. For example, customers in a specific segment may receive targeted promotions or recommendations based on their preferences and purchasing behavior.\n",
    "\n",
    "By applying KNN in this scenario, the retail store can gain insights into customer segments and make data-driven decisions to improve customer satisfaction and drive sales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7263d778",
   "metadata": {},
   "source": [
    "# Clustering:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f1a45",
   "metadata": {},
   "source": [
    "**Ques19.**  What is clustering in machine learning?\n",
    "\n",
    "Clustering is an unsupervised machine learning techniue that aims to group similar instances together based on their patterns or similarites The goal is to identify distinct clusters within a dataset without any prior knowledge of class labels or target variables. Clustering algorithms seek to maximize the similarity within clusters while minimizing the similarity between different clusters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c2376e",
   "metadata": {},
   "source": [
    "**Ques20.** Explain the difference between hierarchical clustering and k-means clustering.\n",
    "\n",
    "1. __Approach:__ Hierarchical clustering builds a hierarchy of clusters, while k-means clustering partitions the data into a fixed number of clusters.\n",
    "2. __Number of Clusters:__ Hierarchical clustering does not require specifying the number of clusters in advance, while k-means clustering requires predefining the number of clusters.\n",
    "3. __Visualization:__ Hierarchical clustering produces a dendrogram to visualize the clustering hierarchy, while k-means clustering does not provide a visual representation of the clustering structure.\n",
    "4. __Cluster Assignments:__ Hierarchical clustering allows instances to be part of multiple levels or subclusters in the hierarchy, while k-means assigns instances to exactly one cluster.\n",
    "5. __Computational Complexity:__ Hierarchical clustering can be computationally expensive for large datasets, while k-means clustering is more computationally efficient.\n",
    "6. __Flexibility:__ Hierarchical clustering allows for exploring clusters at different levels of granularity, while k-means clustering provides fixed partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc54f968",
   "metadata": {},
   "source": [
    "**Ques21.** How do you determine the optimal number of clusters in k-means clustering?\n",
    "\n",
    "1. Elbow Method:\n",
    "   - The Elbow Method involves plotting the within-cluster sum of squared distances (WCSS) against the number of clusters (k).\n",
    "   - WCSS measures the compactness of clusters, and a lower WCSS indicates better clustering.\n",
    "   - The plot resembles an arm, and the \"elbow\" point represents the optimal number of clusters.\n",
    "   - The elbow point is the value of k where the decrease in WCSS begins to level off significantly.\n",
    "   - This method helps identify the value of k where adding more clusters does not provide substantial improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8cad1719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "wcss = []\n",
    "a = np.random.randint(low=1,high=10, size=(100,19))\n",
    "b = np.random.randint(low=20,high=30, size=(100,19))\n",
    "c = np.random.randint(low=40,high=60, size=(100,19))\n",
    "d = np.random.randint(low=90,high=100, size=(100,19))\n",
    "\n",
    "data = np.concatenate((a,b,c,d))\n",
    "\n",
    "for k in range(1,11):\n",
    "    kmeans = KMeans(n_clusters=k,n_init=10)\n",
    "    kmeans.fit(data)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1,11),wcss)\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('WCSS')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcba746",
   "metadata": {},
   "source": [
    "2. Silhouette Analysis:\n",
    "   - Silhouette analysis measures the compactness and separation of clusters.\n",
    "   - It calculates the average silhouette coefficient for each instance, which represents how well it fits within its cluster compared to other clusters.\n",
    "   - The silhouette coefficient ranges from -1 to 1, where values close to 1 indicate well-clustered instances, values close to 0 indicate overlapping instances, and negative values indicate potential misclassifications.\n",
    "   - The optimal number of clusters corresponds to the highest average silhouette coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5295c07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_scores = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k,n_init=10)\n",
    "    kmeans.fit(data)\n",
    "    labels = kmeans.labels_\n",
    "    score = silhouette_score(data, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "plt.plot(range(2, 11), silhouette_scores)\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e352de5",
   "metadata": {},
   "source": [
    "3. Domain Knowledge and Interpretability:\n",
    "   - In some cases, the optimal number of clusters can be determined based on domain knowledge or specific requirements.\n",
    "   - For example, in customer segmentation, a business may decide to have a certain number of distinct customer segments based on their marketing strategies or product offerings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcccdbb0",
   "metadata": {},
   "source": [
    "**Ques22.** What are some common distance metrics used in clustering?\n",
    "\n",
    "1. Euclidean Distance:\n",
    "   - Euclidean distance is the most commonly used distance metric in clustering algorithms.\n",
    "   - It measures the straight-line distance between two instances in the feature space.\n",
    "   - Euclidean distance assumes that all dimensions are equally important and scales linearly.\n",
    "   - It works well when the dataset has continuous numerical features and there are no significant variations in feature scales.\n",
    "   - Euclidean distance tends to produce spherical or convex-shaped clusters.\n",
    "<br><br>\n",
    "2. Manhattan Distance:\n",
    "   - Manhattan distance, also known as city block distance or L1 distance, measures the sum of absolute differences between corresponding coordinates of two instances.\n",
    "   - It calculates the distance as the sum of horizontal and vertical movements needed to move from one instance to another.\n",
    "   - Manhattan distance is suitable when dealing with categorical variables or features with different scales.\n",
    "   - It can produce clusters with different shapes, as it measures the \"taxicab\" distance along the grid lines.\n",
    "<br><br>\n",
    "3. Cosine Distance:\n",
    "   - Cosine distance measures the angle between two instances in the feature space.\n",
    "   - It calculates the cosine of the angle between two vectors, representing their similarity.\n",
    "   - Cosine distance is particularly useful for text or document clustering, where the magnitude of the vector does not matter, only the direction or orientation of the vectors.\n",
    "   - It is insensitive to the scale of the features and captures the similarity of the feature patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dfe02e",
   "metadata": {},
   "source": [
    "**Ques23.** How do you handle categorical features in clustering?\n",
    "\n",
    "Their are various ways to handel categorical variables in the naive approach :-\n",
    "\n",
    "_**Label Encoding**_\n",
    "- In this we give different ranks to each of the category in that feature. it requires domain knowledge for selcting the ranks.\n",
    "- ex = if if have a feature containg category like 'Human', 'cat', 'dog', now according to the label encoding we give 1 to 'Human' , 2 to 'cat' and 3 'dog'\n",
    "- However, this method introduces an arbitrary order to the categories, which may not be appropriate for some features where the order doesn't have any significance.\n",
    "\n",
    "_**One hot encoding**_\n",
    "- In this we make dummy features of all category containg either 0 or 1 on one of these dummy class will be having 1 and others will be having only 0. the dummy feature with 1 will be the class for that data point \n",
    "- ex = if have a feature 'Type' containg category like 'Human', 'cat', 'dog', now according to the one hot encoding we make dummy features as 'type_Human' , 'type_cat' , 'type_dog' and if for a row the type was 'cat' than for that row value of these mentioned dummy classes will be [0,1,0]\n",
    "- One-hot encoding avoids the issue of introducing arbitrary order but can result in a high-dimensional feature space, especially when dealing with a large number of categories.\n",
    "\n",
    "_**Binary Class Encoding**_\n",
    "- in this we use binary represntaion for showing the category.\n",
    "- ex => if have a feature 'Type' containg classes like 'Human', 'cat', 'dog', now according to the Binary Class encoding we make 'Human' as 00 , 'cat' as 01 , 'dog' as 10\n",
    "- the dimensionality compared to one-hot encoding while preserving some information about the categories.\n",
    "\n",
    "\n",
    "_**Count Encoding**_\n",
    "   - Count encoding replaces each category with the count of its occurrences in the dataset.\n",
    " - For example, if we have a feature \"type\" with categories \"Human\", \"cat\" and \"dog\" count encoding would replace them with the respective counts of instances belonging to each type.\n",
    "- This method captures the frequency information of each category and can be useful when the count of occurrences is informative for the classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd022daa",
   "metadata": {},
   "source": [
    "**Ques24.** What are the advantages and disadvantages of hierarchical clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10adecf2",
   "metadata": {},
   "source": [
    "ADVANTAGE :\n",
    "- Robustness: Hierarchical clustering is more robust than other methods since it does not require a predetermined number of clusters to be specified. Instead, it creates hierarchical clusters based on the similarity between the objects, which makes it more reliable and accurate. [0]\n",
    "- Easy to interpret: Hierarchical clustering produces a tree-like structure called a dendrogram that is easy to interpret and understand. This makes it ideal for data analysis as it can provide insights into the data without requiring complex algorithms or deep learning models. [0]\n",
    "- Flexible: Hierarchical clustering is a flexible method that can be used on any type of data. It can also be used with different types of similarity functions and distance measures, allowing for customization based on the application at hand. [0]\n",
    "- Scalable: Hierarchical clustering is a scalable method that can easily handle large datasets without becoming computationally expensive or time-consuming. This makes it suitable for applications such as customer segmentation where large datasets need to be processed quickly and accurately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db97e40",
   "metadata": {},
   "source": [
    "DISADVANTAGE :\n",
    "\n",
    "- Sensitivity to outliers: Hierarchical clustering is sensitive to outliers, and outliers can significantly influence the clusters that are formed. They can even cause incorrect results if the dataset contains these types of data points. [0]\n",
    "- Computationally expensive: Hierarchical clustering can be computationally expensive, especially as the number of data points increases. The time required to run the algorithm increases exponentially, making it difficult to use for large datasets. [0]\n",
    "- Difficulty in interpreting results: The results of hierarchical clustering, especially in the form of dendrograms, can sometimes be difficult to interpret due to their complexity. This can make it challenging to draw meaningful conclusions from the results. [0]\n",
    "- Lack of guaranteed optimal results: Hierarchical clustering does not guarantee optimal results or the best possible clusterings. Since it is an unsupervised learning algorithm, it relies on the researcher's judgment and experience to assess the quality of the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8dac4e",
   "metadata": {},
   "source": [
    "**Ques25.** Explain the concept of silhouette score and its interpretation in clustering.\n",
    "\n",
    "The silhouette score is a measure of how well each object in a clustering algorithm has been classified within its own cluster compared to other clusters. It ranges from -1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. On the other hand, a low or negative value indicates that the object may have been assigned to the wrong cluster. The silhouette score can be calculated using any distance metric, such as Euclidean distance or Manhattan distance\n",
    "\n",
    "The silhouette score can be interpreted as follows:\n",
    "- A score close to 1 indicates that the data point is well-clustered within its own cluster and far away from other clusters.\n",
    "- A score close to -1 indicates that the data point is misclassified and should belong to a neighboring cluster.\n",
    "- Scores near 0 indicate overlapping clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce22268",
   "metadata": {},
   "source": [
    "**Ques26.** Give an example scenario where clustering can be applied.\n",
    "\n",
    "Clustering can be applied in various scenarios where grouping similar data points together is useful. Here is an example scenario where clustering can be applied:\n",
    "\n",
    "Scenario: Customer Segmentation In the field of marketing, clustering can be used for customer segmentation. Customer segmentation involves dividing a customer base into distinct groups based on their characteristics, behaviors, or preferences. By clustering customers into different segments, businesses can better understand their customers, tailor marketing strategies, and provide personalized experiences.\n",
    "\n",
    "For example, a retail company may want to segment its customers based on their purchasing patterns. They can collect data such as customer demographics, purchase history, frequency of purchases, and average order value. By applying clustering algorithms to this data, the company can identify different customer segments, such as high-value customers, occasional buyers, or price-sensitive customers.\n",
    "\n",
    "Once the customer segments are identified, the company can develop targeted marketing campaigns for each segment. For instance, they can offer personalized recommendations, discounts, or loyalty programs to high-value customers to encourage repeat purchases. They can also create specific promotions or discounts to attract price-sensitive customers.\n",
    "\n",
    "Clustering in customer segmentation helps businesses gain insights into their customer base, understand customer preferences, and make data-driven decisions to improve customer satisfaction and increase sales.\n",
    "\n",
    "It's important to note that clustering can be applied in various other scenarios, such as image segmentation, document clustering, anomaly detection, and social network analysis. The specific application of clustering depends on the problem domain and the type of data being analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb273f3e",
   "metadata": {},
   "source": [
    "# Anomaly Detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e7e950",
   "metadata": {},
   "source": [
    "**Ques27.** What is anomaly detection in machine learning?\n",
    "\n",
    "Anomaly detection, also known as outlier detection, is the task of identifying patterns or instances that deviate significantly from the norm or expected behavior within a dataset. Anomalies are data points that differ from the majority of the data and may indicate unusual or suspicious behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a41bc0",
   "metadata": {},
   "source": [
    "**Ques28.** Explain the difference between supervised and unsupervised anomaly detection.\n",
    "\n",
    "1. Supervised Anomaly Detection:\n",
    "   - In supervised anomaly detection, the training dataset contains labeled instances, where each instance is labeled as either normal or anomalous.\n",
    "   - The algorithm learns from these labeled examples to classify new, unseen instances as normal or anomalous.\n",
    "   - Supervised anomaly detection typically involves the use of classification algorithms that are trained on labeled data.\n",
    "   - The algorithm learns the patterns and characteristics of normal instances and uses this knowledge to classify new instances.\n",
    "   - Supervised anomaly detection requires a sufficient amount of labeled data, including both normal and anomalous instances, for training.\n",
    "<br><br>\n",
    "2. Unsupervised Anomaly Detection:\n",
    "   - In unsupervised anomaly detection, the training dataset does not contain any labeled instances. The algorithm learns the normal behavior or patterns solely from the unlabeled data.\n",
    "   - The goal is to identify instances that deviate significantly from the learned normal behavior, considering them as anomalies.\n",
    "   - Unsupervised anomaly detection algorithms rely on the assumption that anomalies are rare and different from the majority of the data.\n",
    "   - These algorithms aim to capture the underlying structure or distribution of the data and detect instances that do not conform to that structure.\n",
    "   - Unsupervised anomaly detection is useful when labeled data for anomalies is scarce or unavailable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c61c285",
   "metadata": {},
   "source": [
    "**Ques29.** What are some common techniques used for anomaly detection?\n",
    "\n",
    "1. Statistical Methods:\n",
    "   - Z-Score: Calculates the standard deviation of the data and identifies instances that fall outside a specified number of standard deviations from the mean.\n",
    "   - Grubbs' Test: Detects outliers based on the maximum deviation from the mean.\n",
    "   - Dixon's Q Test: Identifies outliers based on the difference between the extreme value and the next closest value.\n",
    "   - Box Plot: Visualizes the distribution of the data and identifies instances falling outside the whiskers.\n",
    "<br><br>\n",
    "2. Machine Learning Methods:\n",
    "   - Isolation Forest: Builds an ensemble of isolation trees to isolate instances that are easily separable from the majority of the data.\n",
    "   - One-Class SVM: Constructs a boundary around the normal instances and identifies instances outside this boundary as anomalies.\n",
    "   - Local Outlier Factor (LOF): Measures the local density deviation of an instance compared to its neighbors and identifies instances with significantly lower density as anomalies.\n",
    "   - Autoencoders: Unsupervised neural networks that learn to reconstruct normal instances and flag instances with large reconstruction errors as anomalies.\n",
    "<br><br>\n",
    "3. Density-Based Methods:\n",
    "   - DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Clusters instances based on their density and identifies instances in low-density regions as anomalies.\n",
    "   - LOCI (Local Correlation Integral): Measures the local density around an instance and compares it with the expected density, identifying instances with significantly lower density as anomalies.\n",
    "<br><br>\n",
    "4. Proximity-Based Methods:\n",
    "   - K-Nearest Neighbors (KNN): Identifies instances with few or no neighbors within a specified distance as anomalies.\n",
    "   - Local Outlier Probability (LoOP): Assigns an anomaly score based on the distance to its kth nearest neighbor and the density of the region.\n",
    "<br><br>\n",
    "5. Time-Series Specific Methods:\n",
    "   - ARIMA: Models the time series data and identifies instances with large residuals as anomalies.\n",
    "   - Seasonal Hybrid ESD (Extreme Studentized Deviate): Identifies anomalies in seasonal time series data by considering seasonality and decomposing the time series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af31bd0f",
   "metadata": {},
   "source": [
    "**Ques30.** How does the One-Class SVM algorithm work for anomaly detection?\n",
    "\n",
    "1. Training Phase:\n",
    "   - The One-Class SVM algorithm is trained on a dataset that contains only normal instances, without any labeled anomalies.\n",
    "   - The algorithm learns the boundary that encapsulates the normal instances and aims to maximize the margin around them.\n",
    "   - The hyperplane is determined by a subset of the training instances called support vectors, which lie closest to the separating boundary.\n",
    "<br><br>\n",
    "2. Testing Phase:\n",
    "   - During the testing phase, new instances are evaluated to determine if they belong to the normal class or if they are anomalous.\n",
    "   - The One-Class SVM assigns a decision function value to each instance, indicating its proximity to the learned boundary.\n",
    "   - Instances that fall within the decision function values are considered normal, while instances outside the decision function values are considered anomalous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03489f92",
   "metadata": {},
   "source": [
    "**Ques31.** How do you choose the appropriate threshold for anomaly detection?\n",
    "\n",
    "Choosing the threshold for detecting anomalies depends on the desired trade-off between false positives and false negatives, which can vary based on the specific application and requirements. Here are a few approaches to choosing the threshold for detecting anomalies:\n",
    "\n",
    "1. Statistical Methods:\n",
    "   - Empirical Rule: In a normal distribution, approximately 68% of the data falls within one standard deviation, 95% falls within two standard deviations, and 99.7% falls within three standard deviations. You can use these percentages as thresholds to classify instances as anomalies.\n",
    "   - Percentile: You can choose a specific percentile of the anomaly score distribution as the threshold. For example, you can set the threshold at the 95th percentile to capture the top 5% of the most anomalous instances.\n",
    "<br><br>\n",
    "2. Domain Knowledge:\n",
    "   - Domain expertise can play a crucial role in determining the threshold. Based on the specific problem domain, you may have prior knowledge or business rules that define what constitutes an anomaly. You can set the threshold accordingly.\n",
    "<br><br>\n",
    "3. Validation Set or Cross-Validation:\n",
    "   - You can reserve a portion of your labeled data as a validation set or use cross-validation techniques to evaluate different thresholds and choose the one that optimizes the desired performance metric, such as precision, recall, or F1 score.\n",
    "   - By trying different threshold values and evaluating the performance on the validation set, you can identify the threshold that achieves the best balance between false positives and false negatives.\n",
    "<br><br>\n",
    "4. Anomaly Score Distribution:\n",
    "   - Analyzing the distribution of anomaly scores can provide insights into the separation between normal and anomalous instances. You can visually examine the distribution and choose a threshold that appears to appropriately separate the two groups.\n",
    "<br><br>\n",
    "5. Cost-Based Analysis:\n",
    "   - Consider the costs associated with false positives and false negatives in your specific application. Assign different costs to each type of error and choose the threshold that minimizes the overall cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9b4f28",
   "metadata": {},
   "source": [
    "**Ques32.** How do you handle imbalanced datasets in anomaly detection?\n",
    "\n",
    "- challenge: Anomalies are often rare compared to normal instances, leading to imbalanced datasets with a significant class imbalance.\n",
    "- Solution: Techniques such as oversampling, undersampling, or synthetic data generation can be used to balance the dataset. Additionally, adjusting the threshold or using anomaly detection algorithms specifically designed for imbalanced data, like anomaly detection with imbalanced learning (ADIL), can help handle imbalanced datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509d99fa",
   "metadata": {},
   "source": [
    "**Ques33.** Give an example scenario where anomaly detection can be applied.\n",
    "\n",
    "Anomaly detection is useful in various scenarios where detecting unusual or anomalous patterns is crucial for maintaining system integrity, identifying fraud, or ensuring safety. One example scenario where anomaly detection is valuable is in cybersecurity:\n",
    "\n",
    "Scenario: Network Intrusion Detection\n",
    "In an organization's network infrastructure, an anomaly detection system is implemented to monitor network traffic and detect potential security breaches or unauthorized activities.\n",
    "\n",
    "Anomaly Detection Application:\n",
    "The anomaly detection system analyzes network traffic data in real-time, comparing it to historical patterns and known behavior. It identifies any deviations or anomalies that may indicate network intrusions, malware infections, or suspicious activities.\n",
    "\n",
    "Anomaly Detection Techniques:\n",
    "1. Statistical Methods: Statistical analysis is performed on various network traffic attributes, such as packet sizes, communication patterns, or protocol usage. Deviations from expected statistical distributions or sudden spikes in traffic can indicate anomalous behavior.\n",
    "\n",
    "2. Machine Learning Approaches: Machine learning algorithms, such as clustering, classification, or deep learning models, are trained on historical network traffic data. These models can identify patterns of normal network behavior and detect anomalies by comparing new data points to the learned patterns.\n",
    "\n",
    "3. Signature-Based Detection: Known patterns of network attacks or intrusion signatures are used to identify specific types of anomalies. This approach relies on a database of known attack patterns or malicious indicators to match against the network traffic data.\n",
    "\n",
    "4. Behavioral Analysis: The system continuously learns the normal behavior of network traffic and devices. It detects anomalies by flagging deviations from the learned behavior, such as unexpected communication patterns, unusual data transfers, or unauthorized access attempts.\n",
    "\n",
    "Example:\n",
    "Suppose the anomaly detection system observes a sudden increase in outgoing network traffic from an employee's workstation during non-working hours. This unusual behavior is flagged as an anomaly. Upon investigation, it is discovered that the employee's workstation was compromised, and unauthorized data exfiltration was taking place. The anomaly detection system detected this abnormal behavior, enabling timely response and preventing potential data breaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54fbbe",
   "metadata": {},
   "source": [
    "# Dimension Reduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d25a888",
   "metadata": {},
   "source": [
    "**Ques34.** What is dimension reduction in machine learning?\n",
    "\n",
    "Dimensionality reduction is a technique used in machine learning to reduce the number of input features or variables while preserving the most relevant information. It aims to simplify the data representation, remove noise or irrelevant features, and improve computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c50a64",
   "metadata": {},
   "source": [
    "**Ques35.** Explain the difference between feature selection and feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009f8a2",
   "metadata": {},
   "source": [
    "__Feature Selection__\n",
    "1. Subset of Features: Feature selection focuses on identifying a subset of the original features that are most predictive or have the strongest relationship with the target variable.\n",
    "\n",
    "2. Retains Original Features: Feature selection retains the original features and their values. It does not modify or transform the feature values.\n",
    "\n",
    "3. Criteria for Selection: Various criteria can be used for feature selection, such as statistical measures (e.g., correlation, mutual information), feature importance rankings (e.g., based on tree-based models), or domain knowledge.\n",
    "\n",
    "4. Benefits: Feature selection improves model interpretability, reduces overfitting, and enhances computational efficiency by working with a reduced set of features.\n",
    "|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ad8e22",
   "metadata": {},
   "source": [
    "__Feature Extraction__\n",
    "\n",
    "rojections, or transformations of the original features. These derived features may not have a direct correspondence to the original features.\n",
    "\n",
    "2. Dimensionality Reduction: Feature extraction techniques aim to reduce the dimensionality of the data by representing it in a lower-dimensional space while preserving important patterns or structures.\n",
    "\n",
    "3. Data Transformation: Feature extraction involves applying mathematical or statistical operations to transform the original feature values into new representations.\n",
    "\n",
    "4. Benefits: Feature extraction helps in handling multicollinearity, capturing latent factors, and reducing the complexity of high-dimensional data. It can also improve model performance and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8637dd85",
   "metadata": {},
   "source": [
    "**Ques36.** How does Principal Component Analysis (PCA) work for dimension reduction?\n",
    "\n",
    "1. Standardize the Data:\n",
    "   - PCA requires the data to be standardized, i.e., mean-centered with unit variance. This step ensures that variables with larger scales do not dominate the analysis.\n",
    "<br><br>\n",
    "2. Compute the Covariance Matrix:\n",
    "   - Calculate the covariance matrix of the standardized data, which represents the relationships and variances among the variables.\n",
    "<br><br>\n",
    "3. Calculate the Eigenvectors and Eigenvalues:\n",
    "   - Obtain the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the directions or axes in the data with the highest variance, and eigenvalues correspond to the amount of variance explained by each eigenvector.\n",
    "<br><br>\n",
    "4. Select Principal Components:\n",
    "   - Sort the eigenvectors in descending order based on their corresponding eigenvalues. The eigenvectors with the highest eigenvalues capture the most variance in the data.\n",
    "   - Choose the top-k eigenvectors (principal components) that explain a significant portion of the total variance. Typically, a cutoff based on the cumulative explained variance or a desired level of retained variance is used.\n",
    "<br><br>\n",
    "5. Project the Data:\n",
    "   - Project the standardized data onto the selected principal components to obtain a reduced-dimensional representation of the original data.\n",
    "   - The new set of variables (principal components) are uncorrelated with each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e550b0b",
   "metadata": {},
   "source": [
    "**Ques37.** How do you choose the number of components in PCA?\n",
    "\n",
    "1. Variance Explained:\n",
    "   - Calculate the cumulative explained variance ratio for each principal component. This indicates the proportion of total variance captured by including that component. Choose the number of components that sufficiently explain the desired amount of variance, such as 90% or 95%.\n",
    "   - Example: Plot the cumulative explained variance ratio against the number of components and select the number at which the curve levels off or reaches the desired threshold.\n",
    "<br><br>\n",
    "2. Elbow Method:\n",
    "   - Plot the explained variance as a function of the number of components. Look for an \"elbow\" point where the explained variance starts to level off. This suggests that adding more components beyond that point does not contribute significantly to the overall variance explained.\n",
    "   - Example: Plot the explained variance against the number of components and select the number at the elbow point.\n",
    "<br><br>\n",
    "3. Scree Plot:\n",
    "   - Plot the eigenvalues of the principal components in descending order. Look for a point where the eigenvalues drop sharply, indicating a significant drop in explained variance. The number of components corresponding to that point can be chosen.\n",
    "   - Example: Plot the eigenvalues against the number of components and select the number where the drop is significant.\n",
    "<br><br>\n",
    "4. Cross-validation:\n",
    "   - Use cross-validation techniques to evaluate the performance of the PCA with different numbers of components. Select the number of components that maximizes a performance metric, such as model accuracy or mean squared error, on the validation set.\n",
    "   - Example: Implement k-fold cross-validation with varying numbers of components and select the number that results in the best performance metric on the validation set.\n",
    "<br><br>\n",
    "5. Domain Knowledge and Task Specificity:\n",
    "   - Consider the specific requirements of the task and the domain. Depending on the application, you may have prior knowledge or constraints that guide the selection of the number of components.\n",
    "   - Example: In some cases, there may be a known intrinsic dimensionality or specific requirements for interpretability, computational efficiency, or feature space reduction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a838d1",
   "metadata": {},
   "source": [
    "**Ques38.** What are some other dimension reduction techniques besides PCA?\n",
    "\n",
    "1. Linear Discriminant Analysis (LDA):\n",
    "   - LDA is a supervised dimensionality reduction technique that aims to find a lower-dimensional representation of the data that maximizes the separation between different classes or groups.\n",
    "   - It computes the linear combinations of the original features that maximize the between-class scatter while minimizing the within-class scatter.\n",
    "   - LDA is commonly used in classification tasks where the goal is to maximize the separability of different classes.\n",
    "<br><br>\n",
    "2. t-SNE (t-Distributed Stochastic Neighbor Embedding):\n",
    "   - t-SNE is a non-linear dimensionality reduction technique that is particularly effective in visualizing high-dimensional data in a lower-dimensional space.\n",
    "   - It focuses on preserving the local structure of the data, aiming to represent similar instances as close neighbors and dissimilar instances as distant neighbors.\n",
    "   - t-SNE is often used for data visualization and exploratory analysis, revealing hidden patterns and clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e918fb7",
   "metadata": {},
   "source": [
    "**Ques39.** Give an example scenario where dimension reduction can be applied.\n",
    "\n",
    "__*Scenario:*__ Image Processing In the field of image processing, dimension reduction techniques can be used to reduce the dimensionality of image data while preserving important visual information. This is particularly useful when working with high-resolution images or large image datasets.\n",
    "\n",
    "For example, consider a scenario where a computer vision system needs to analyze a large collection of images for object recognition or image classification tasks. Each image in the dataset may consist of thousands or millions of pixels, resulting in a high-dimensional feature space. However, not all pixels contribute equally to the overall visual content or discriminative features of the images.\n",
    "\n",
    "By applying dimension reduction techniques such as Principal Component Analysis (PCA) or t-SNE (t-Distributed Stochastic Neighbor Embedding), it is possible to reduce the dimensionality of the image data while retaining the most important visual information. These techniques identify the most informative features or patterns in the images and project them onto a lower-dimensional space.\n",
    "\n",
    "The reduced-dimensional representation of the images can then be used for various purposes, such as visualization, clustering, or classification. For instance, the reduced-dimensional representation can be visualized in a scatter plot, where similar images are grouped together based on their visual similarity. This can provide insights into the structure or patterns present in the image dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0519c2a8",
   "metadata": {},
   "source": [
    "# Feature Selection:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb54591d",
   "metadata": {},
   "source": [
    "**Ques40.** What is feature selection in machine learning?\n",
    "\n",
    "Feature selection is the process of selecting a subset of relevant features from a larger set of available features in a machine learning dataset. The goal of feature selection is to improve model performance, reduce complexity, enhance interpretability, and mitigate the risk of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d39edba",
   "metadata": {},
   "source": [
    "**Ques41.** Explain the difference between filter, wrapper, and embedded methods of feature selection.\n",
    "\n",
    "1. Filter Methods:\n",
    "   - Filter methods are based on statistical measures and evaluate the relevance of features independently of any specific machine learning algorithm.\n",
    "   - They rank or score features based on certain statistical metrics, such as correlation, mutual information, or statistical tests like chi-square or ANOVA.\n",
    "   - Features are selected or ranked based on their individual scores, and a threshold is set to determine the final subset of features.\n",
    "   - Filter methods are computationally efficient and can be applied as a preprocessing step before applying any machine learning algorithm.\n",
    "   - However, they do not consider the interaction or dependency between features or the impact of feature subsets on the performance of the specific learning algorithm.\n",
    "<br><br>\n",
    "2. Wrapper Methods:\n",
    "   - Wrapper methods evaluate subsets of features by training and evaluating the model performance with different feature combinations.\n",
    "   - They use a specific machine learning algorithm as a black box and assess the quality of features by directly optimizing the performance of the model.\n",
    "   - Wrapper methods involve an iterative search process, exploring different combinations of features and evaluating them using cross-validation or other performance metrics.\n",
    "   - They consider the interaction and dependency between features, as well as the specific learning algorithm, but can be computationally expensive due to the repeated training of the model for different feature subsets.\n",
    "<br><br>\n",
    "3. Embedded Methods:\n",
    "   - Embedded methods incorporate feature selection within the model training process itself.\n",
    "   - They select features as part of the model training algorithm, where the selection is driven by some internal criteria or regularization techniques.\n",
    "   - Examples include L1 regularization (Lasso) in linear models, which simultaneously performs feature selection and model fitting.\n",
    "   - Embedded methods are computationally efficient since feature selection is combined with the training process, but the selection depends on the specific algorithm and its inherent feature selection mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089eca5",
   "metadata": {},
   "source": [
    "**Ques42.** How does correlation-based feature selection work?\n",
    "\n",
    "Correlation-based feature selection is a filter method used to select features based on their correlation with the target variable. It assesses the relationship between each feature and the target variable to determine their relevance. Here's how it works:\n",
    "\n",
    "1. Compute Correlation: Calculate the correlation coefficient (e.g., Pearson's correlation) between each feature and the target variable. The correlation coefficient measures the strength and direction of the linear relationship between two variables.\n",
    "\n",
    "2. Select Features: Choose a threshold value for the correlation coefficient. Features with correlation coefficients above the threshold are considered highly correlated with the target variable and are selected as relevant features. Features below the threshold are considered less correlated and are discarded.\n",
    "\n",
    "3. Handle Multicollinearity: If there are highly correlated features among the selected set, further analysis is needed to handle multicollinearity. Redundant features may be removed, or advanced techniques such as principal component analysis (PCA) can be applied to reduce the dimensionality while retaining the information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa75b2fd",
   "metadata": {},
   "source": [
    "**Ques43.** How do you handle multicollinearity in feature selection?\n",
    "\n",
    "1. Remove One of the Correlated Features: If two or more features exhibit a high correlation, you can remove one of them from the feature set. The choice of which feature to remove can be based on domain knowledge, practical considerations, or further analysis of their individual relationships with the target variable.\n",
    "\n",
    "2. Use Dimension Reduction Techniques: Dimension reduction techniques like Principal Component Analysis (PCA) can be applied to create a smaller set of uncorrelated features, known as principal components. PCA transforms the original features into a new set of linearly uncorrelated variables while preserving most of the variance in the data. You can then select the principal components as the representative features.\n",
    "\n",
    "3. Regularization Techniques: Regularization methods, such as L1 regularization (Lasso) and L2 regularization (Ridge), can help mitigate multicollinearity. These techniques introduce a penalty term in the model training process that encourages smaller coefficients for less important features. By shrinking the coefficients, they effectively reduce the impact of correlated features on the model.\n",
    "\n",
    "4. Variance Inflation Factor (VIF): VIF is a metric used to quantify the extent of multicollinearity in a regression model. It measures how much the variance of the estimated regression coefficients is inflated due to multicollinearity. Features with high VIF values indicate a strong correlation with other features. You can assess the VIF for each feature and consider removing features with excessively high VIF values (e.g., VIF > 5 or 10)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc2ffa",
   "metadata": {},
   "source": [
    "**Ques44.** What are some common feature selection metrics?\n",
    "\n",
    "1. Correlation: Correlation measures the linear relationship between two variables. It can be used to assess the correlation between each feature and the target variable. Features with higher absolute correlation coefficients are considered more relevant. For example, Pearson's correlation coefficient is commonly used for continuous variables, while point biserial correlation is used for a binary target variable.\n",
    "\n",
    "2. Mutual Information: Mutual information measures the amount of information shared between two variables. It quantifies the mutual dependence between a feature and the target variable. Higher mutual information indicates a stronger relationship and higher relevance. It is commonly used for both continuous and categorical variables.\n",
    "\n",
    "3. ANOVA (Analysis of Variance): ANOVA assesses the statistical significance of the differences in means across different groups or categories. It can be used to compare the mean values of each feature across different classes or the target variable. Features with significant differences in means are considered more relevant. ANOVA is commonly used for continuous features and categorical target variables.\n",
    "\n",
    "4. Chi-square: Chi-square test measures the association between two categorical variables. It can be used to assess the relationship between each feature and a categorical target variable. Features with higher chi-square statistics and lower p-values are considered more relevant.\n",
    "\n",
    "5. Information Gain: Information gain is a metric used in decision tree-based algorithms. It measures the reduction in entropy or impurity when a feature is used to split the data. Features with higher information gain are considered more informative for classification tasks.\n",
    "\n",
    "6. Gini Importance: Gini importance is another metric used in decision tree-based algorithms, such as Random Forest. It measures the total reduction in the Gini impurity when a feature is used to split the data. Features with higher Gini importance scores are considered more important for classification tasks.\n",
    "\n",
    "7. Recursive Feature Elimination (RFE): RFE is an iterative feature selection approach that assigns importance weights to each feature based on the performance of the model. Features with lower importance weights are eliminated iteratively until the desired number of features is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4480bc6",
   "metadata": {},
   "source": [
    "**Ques45.** Give an example scenario where feature selection can be applied.\n",
    "\n",
    "One example scenario where feature selection is beneficial is in text classification tasks. Consider a problem where you have a large dataset of text documents and you want to classify them into different categories, such as spam detection or sentiment analysis. Each document is represented by a set of features, which could be word counts, TF-IDF values, or other text-based features.\n",
    "\n",
    "In this case, feature selection can be highly beneficial for several reasons:\n",
    "\n",
    "1. Dimensionality Reduction: Text data often results in high-dimensional feature spaces, where each word or term becomes a feature. The high dimensionality can lead to computational inefficiency and the curse of dimensionality. Feature selection allows you to reduce the number of features, focusing on the most relevant ones, thereby simplifying the model and improving computational efficiency.\n",
    "\n",
    "2. Noise Reduction: Text data can contain noisy features, such as rare or irrelevant words, misspellings, or stopwords. Including these noisy features can negatively impact the model's performance and generalization ability. Feature selection helps eliminate such noisy features, enhancing the signal-to-noise ratio and improving the model's performance.\n",
    "\n",
    "3. Interpretability: In text classification, it's often important to understand the key features or terms that contribute most to the classification. Feature selection allows you to identify the most informative features, providing insights into the important words or phrases associated with each class. This enhances the interpretability of the model and helps extract meaningful insights from the text data.\n",
    "\n",
    "4. Overfitting Prevention: Including too many features in the model can increase the risk of overfitting, especially when the number of samples is limited. Feature selection helps mitigate overfitting by reducing the complexity of the model and focusing on the most informative features, improving the model's generalization performance.\n",
    "\n",
    "For example, in spam detection, feature selection can help identify the most discriminative words or patterns that distinguish spam emails from legitimate ones. By selecting relevant features, the model can focus on key indicators of spam and ignore irrelevant or noisy words, leading to improved accuracy and efficiency.\n",
    "\n",
    "Overall, feature selection in text classification tasks helps improve model performance, reduce dimensionality, enhance interpretability, and mitigate overfitting, making it a valuable technique for extracting meaningful insights from text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d9a704",
   "metadata": {},
   "source": [
    "# Data Drift Detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358aa69",
   "metadata": {},
   "source": [
    "**Ques46.** What is data drift in machine learning?\n",
    "\n",
    "Data drift refers to the phenomenon where the statistical properties of the target variable or input features change over time, leading to a degradation in model performance. It is important to monitor and address data drift in machine learning because models trained on historical data may become less accurate or unreliable when deployed in production environments where the underlying data distribution has changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02adc175",
   "metadata": {},
   "source": [
    "**Ques47.** Why is data drift detection important?\n",
    "\n",
    "Here are a few examples to illustrate the importance of detecting and handling data drift:\n",
    "\n",
    "1. Customer Behavior: Consider a customer churn prediction model that has been trained on historical customer data. Over time, customer preferences, behaviors, or market conditions may change, leading to shifts in customer behavior. If these changes are not accounted for, the churn prediction model may lose its accuracy and fail to identify the changing patterns associated with customer churn.\n",
    "\n",
    "2. Fraud Detection: In fraud detection models, patterns of fraudulent activities may change as fraudsters evolve their techniques to avoid detection. If the model is not regularly updated to adapt to these changes, it may become less effective in identifying new fraud patterns, allowing fraudulent activities to go undetected.\n",
    "\n",
    "3. Financial Time Series: Models predicting stock prices or financial indicators rely on historical data patterns. However, market conditions, economic factors, or geopolitical events can cause shifts in the underlying dynamics of financial time series. Failure to account for these changes can lead to inaccurate predictions and financial losses.\n",
    "\n",
    "4. Natural Language Processing: Language is dynamic, and the usage of words, phrases, or sentiment can evolve over time. Models trained on outdated language patterns may struggle to accurately understand and process new text data, leading to degraded performance in tasks such as sentiment analysis or text classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec265091",
   "metadata": {},
   "source": [
    "**Ques48.** Explain the difference between concept drift and feature drift.\n",
    "\n",
    "Feature drift and concept drift are two important concepts related to data drift in machine learning.\n",
    "\n",
    "__Feature Drift:__\n",
    "- Feature drift refers to the change in the distribution or characteristics of individual features over time. It occurs when the statistical properties of the input features used for modeling change or evolve. Feature drift can occur due to various reasons, such as changes in the data collection process, changes in the underlying population, or external factors influencing the feature values.\n",
    "\n",
    "- For example, consider a predictive maintenance system that monitors temperature, pressure, and vibration levels of industrial machines. Over time, the sensors used to collect these features may degrade or require recalibration, leading to changes in the measured values. This results in feature drift, where the statistical properties of the features change, potentially impacting the model's performance.\n",
    "\n",
    "__Concept Drift:__\n",
    "- Concept drift refers to the change in the relationship between input features and the target variable over time. It occurs when the underlying concept or pattern that the model aims to capture evolves or shifts. Concept drift can be caused by changes in user behavior, market dynamics, or external factors influencing the relationship between features and the target variable.\n",
    "\n",
    "- For example, in a customer churn prediction model, the factors influencing customer churn may change over time. This could be due to changes in customer preferences, competitor strategies, or economic conditions. As a result, the model trained on historical data may become less accurate as the underlying concept of churn evolves, leading to concept drift.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f3ae5",
   "metadata": {},
   "source": [
    "**Ques49.** What are some techniques used for detecting data drift?\n",
    "\n",
    "1. Statistical Tests: Statistical tests can be employed to compare the distributions or statistical properties of the data at different time points. For example, the Kolmogorov-Smirnov test, t-test, or chi-square test can be used to assess if there are significant differences in the data distributions. If the test results indicate statistical significance, it suggests the presence of data drift.\n",
    "\n",
    "2. Drift Detection Metrics: Various metrics have been developed specifically for detecting and quantifying data drift. These metrics compare the dissimilarity or distance between two datasets. Examples include the Kullback-Leibler (KL) divergence, Jensen-Shannon divergence, or Wasserstein distance. Higher values of these metrics indicate greater data drift.\n",
    "\n",
    "3. Control Charts: Control charts are graphical tools that help visualize data drift over time. By plotting key statistical measures such as means, variances, or percentiles of the data, control charts can detect significant deviations from the expected behavior. If data points consistently fall outside control limits or show patterns of change, it suggests the presence of data drift.\n",
    "\n",
    "4. Window-Based Monitoring: In this approach, a sliding window of recent data is used to compare against a reference window of stable data. Statistical measures or metrics are calculated for each window, and deviations between the two windows indicate data drift. Examples include the CUSUM algorithm, Exponentially Weighted Moving Average (EWMA), or Sequential Probability Ratio Test (SPRT).\n",
    "\n",
    "5. Ensemble Methods: Ensemble methods combine predictions from multiple models or algorithms trained on different time periods or subsets of the data. By comparing the ensemble's performance over time, discrepancies or degradation in model performance can indicate data drift.\n",
    "\n",
    "6. Monitoring Feature Drift: Monitoring individual features or feature combinations can help detect feature-specific drift. Statistical tests or drift detection metrics can be applied to each feature independently or to the relationship between features. Significant changes suggest feature drift.\n",
    "\n",
    "7. Expert Knowledge and Business Rules: Expert domain knowledge and business rules can also play a crucial role in detecting data drift. Subject matter experts or stakeholders can identify unexpected changes or deviations based on their understanding of the data and business context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d82c26f",
   "metadata": {},
   "source": [
    "**Ques50.** How can you handle data drift in a machine learning model?\n",
    "\n",
    "1. Regular Model Retraining: One approach is to periodically retrain the machine learning model using updated data. By including recent data, the model can adapt to the changing data distribution and capture any new patterns or relationships. This helps in mitigating the impact of data drift.\n",
    "\n",
    "2. Incremental Learning: Instead of retraining the entire model from scratch, incremental learning techniques can be used. These techniques update the model incrementally by incorporating new data while preserving the knowledge gained from previous training. Online learning algorithms, such as stochastic gradient descent, are commonly used for incremental learning.\n",
    "\n",
    "3. Drift Detection and Model Updates: Implementing drift detection algorithms allows the model to detect changes in data distribution or performance. When significant drift is detected, the model can trigger an update or retraining process. For example, if the model's prediction accuracy drops below a certain threshold or if statistical tests indicate significant differences in data distributions, it can signal the need for model updates.\n",
    "\n",
    "4. Ensemble Methods: Ensemble techniques can help in handling data drift by combining predictions from multiple models. This can be achieved by training separate models on different time periods or subsets of data. By aggregating predictions from these models, the ensemble can adapt to the changing data distribution and improve overall performance.\n",
    "\n",
    "5. Data Augmentation and Synthesis: Data augmentation techniques can be employed to generate synthetic data that resembles the newly encountered data distribution. This can help in expanding the training dataset and reducing the impact of data drift. Techniques like SMOTE (Synthetic Minority Over-sampling Technique) or generative models like Variational Autoencoders (VAEs) can be used for data augmentation.\n",
    "\n",
    "6. Transfer Learning: Transfer learning involves leveraging knowledge learned from a related task or dataset to improve model performance on a target task. By utilizing pre-trained models or features extracted from similar domains, the model can adapt to new data distributions more effectively.\n",
    "\n",
    "7. Monitoring and Feedback Loops: Implementing monitoring systems to track model performance and data characteristics is crucial. Regularly monitoring predictions, evaluation metrics, and data statistics can help detect drift early on. Feedback loops between model predictions and ground truth can provide valuable insights for identifying and addressing data drift.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e2687",
   "metadata": {},
   "source": [
    "# Data Leakage:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6dcf33",
   "metadata": {},
   "source": [
    "**Ques51.** What is data leakage in machine learning?\n",
    "\n",
    "Data leakage refers to the unintentional or improper inclusion of information from the training data that should not be available during the model's deployment or evaluation. It occurs when there is a contamination of the training data with information that is not realistically obtainable at the time of prediction or when evaluating model performance. Data leakage can significantly impact the accuracy and reliability of machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885bd94e",
   "metadata": {},
   "source": [
    "**Ques52.** Why is data leakage a concern?\n",
    "\n",
    "Data leakage is a concern in machine learning because it leads to overly optimistic performance estimates during model development, making the model seem more accurate than it actually is. When deployed in the real world, the model is likely to perform poorly, resulting in inaccurate predictions, unreliable insights, and potential financial or operational consequences. To mitigate data leakage, it is crucial to carefully analyze the data, ensure proper separation of training and evaluation data, follow best practices in feature engineering and preprocessing, and maintain a strict focus on preserving the integrity of the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73676431",
   "metadata": {},
   "source": [
    "**Ques53.** Explain the difference between target leakage and train-test contamination.\n",
    "\n",
    "__Target Leakage:__\n",
    "- Target leakage refers to the situation where information from the target variable is unintentionally included in the feature set. This means that the feature includes data that would not be available at the time of making predictions in real-world scenarios.\n",
    "- Target leakage leads to inflated performance during model training and evaluation because the model has access to information that it would not realistically have during deployment.\n",
    "- Target leakage can occur when features are derived from data that is generated after the target variable is determined. It can also occur when features are derived using future information or directly encode the target variable.\n",
    "- Examples of target leakage include including the outcome of an event that occurs after the prediction time or using data that is influenced by the target variable to create features.\n",
    "\n",
    "__Train-Test Contamination:__\n",
    "- Train-test contamination occurs when information from the test set (unseen data) leaks into the training set (used for model training).\n",
    "- Train-test contamination leads to overly optimistic performance estimates during model development because the model has \"seen\" the test data and can learn from it, which is not representative of real-world scenarios.\n",
    "- Train-test contamination can occur due to improper splitting of the data, where the test set is inadvertently used during feature engineering, model selection, or hyperparameter tuning.\n",
    "- Train-test contamination can also occur when data preprocessing steps, such as scaling or normalization, are applied to the entire dataset before splitting it into train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2b3280",
   "metadata": {},
   "source": [
    "**Ques54.** How can you identify and prevent data leakage in a machine learning pipeline?\n",
    "\n",
    "Here are some approaches to identify and prevent data leakage in a machine learning pipeline:\n",
    "\n",
    "1. Thoroughly Understand the Data: Gain a deep understanding of the data and the problem domain. Identify potential sources of leakage and determine which variables should be used as predictors and which should be excluded.\n",
    "\n",
    "2. Follow Proper Data Splitting: Split the data into distinct training, validation, and test sets. Ensure that the test set remains completely separate and is not used during model development and evaluation.\n",
    "\n",
    "3. Examine Feature Engineering Steps: Review feature engineering steps carefully to identify any potential sources of leakage. Ensure that feature engineering is performed only on the training data and not influenced by the target variable or future information.\n",
    "\n",
    "4. Validate Feature Importance: If using feature selection techniques, validate the importance of selected features on an independent validation set. This helps confirm that feature selection is based on information available only during training.\n",
    "\n",
    "5. Pay Attention to Time-Based Data: If the data has a temporal component, be cautious about including features that would not be available at the time of prediction. Consider using a rolling window approach or incorporating time-lagged variables appropriately.\n",
    "\n",
    "6. Monitor Performance on Validation Set: Continuously monitor the performance of the model on the validation set during development. Sudden or unexpected jumps in performance can be indicative of data leakage.\n",
    "\n",
    "7. Conduct Cross-Validation Properly: If using cross-validation, ensure that each fold is treated as an independent evaluation set. Feature engineering and data preprocessing should be performed within each fold separately.\n",
    "\n",
    "8. Validate with Real-world Scenarios: Before deploying the model, validate its performance on a separate, unseen dataset that closely resembles the real-world scenario. This helps identify any potential issues related to data leakage or model performance.\n",
    "\n",
    "9. Maintain Data Integrity: Regularly review and update the data pipeline to ensure that no new sources of data leakage are introduced as the project progresses. Consider implementing data monitoring and validation mechanisms to detect and prevent data leakage in real-time.\n",
    "\n",
    "By implementing these steps, data scientists can proactively identify and prevent data leakage in machine learning pipelines, resulting in more reliable and accurate models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcb72c2",
   "metadata": {},
   "source": [
    "**Ques55.** What are some common sources of data leakage?\n",
    "\n",
    "Data leakage can occur due to various sources and scenarios. Here are some common sources of data leakage in machine learning:\n",
    "\n",
    "1. Target Leakage: Including features that are derived from information that would not be available at the time of prediction. For example, including future information or data that is influenced by the target variable can lead to target leakage.\n",
    "\n",
    "2. Time-Based Leakage: Incorporating time-dependent information that should not be available during prediction. This can happen when using future values or time-dependent features that reveal future information.\n",
    "\n",
    "3. Data Preprocessing: Improperly applying preprocessing steps to the entire dataset before splitting into train and test sets. This can include scaling, normalization, or other transformations that introduce information from the test set into the training set.\n",
    "\n",
    "4. Train-Test Contamination: Inadvertently using information from the test set during feature engineering, model selection, or hyperparameter tuning. This can happen when the test set is accidentally accessed or when information leaks from the test set into the training set.\n",
    "\n",
    "5. Data Transformation: Using data-driven transformations or encodings based on the entire dataset, including information that is not available during prediction. This can introduce biases and lead to overfitting.\n",
    "\n",
    "6. Information Leakage: Including features that directly or indirectly reveal information about the target variable. For example, including identifiers or variables that are highly correlated with the target variable.\n",
    "\n",
    "7. Leakage through External Data: Incorporating external data that contains information about the target variable or related features that are not supposed to be available during prediction.\n",
    "\n",
    "8. Human Errors: Mistakenly including data or features that should not be part of the training set, such as accidentally including data points from the future or using confidential data.\n",
    "\n",
    "It is essential to be aware of these potential sources of data leakage and take preventive measures to ensure the integrity and reliability of machine learning models. Thorough understanding of the problem domain, careful data preprocessing, proper data splitting, and vigilant feature engineering are key to avoiding data leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbf2d7e",
   "metadata": {},
   "source": [
    "**Ques56.** Give an example scenario where data leakage can occur.\n",
    "\n",
    "Let's say you're building a credit risk model to predict whether a customer is likely to default on their loan. You have a dataset that includes various features such as income, age, credit score, and employment status. One of the variables in the dataset is \"Payment History,\" which indicates whether the customer has made previous loan payments on time or not.\n",
    "\n",
    "Now, in this scenario, data leakage can occur if you mistakenly include future information about the payment history of the customer in your model. For example, if you have access to the customer's payment history for the current loan, but you inadvertently include their payment history for a future loan that they have not yet taken out, it would lead to data leakage.\n",
    "\n",
    "By including future payment history, the model would have access to information that is not available at the time of prediction. This could result in an artificially high accuracy or performance metrics during model evaluation, as the model would be leveraging future information to make predictions. However, when deploying the model in real-world scenarios, where future payment history is unknown, it would perform poorly and fail to generalize.\n",
    "\n",
    "To prevent data leakage in this scenario, it is essential to ensure that the payment history variable only includes information available up until the time of prediction. Any future payment history data should be excluded from the modeling process to maintain the integrity and reliability of the model.\n",
    "\n",
    "Overall, this example highlights the importance of being vigilant and avoiding the inclusion of information that would not be available during real-world predictions to prevent data leakage and build reliable machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae3e431",
   "metadata": {},
   "source": [
    "# Cross Validation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dce441",
   "metadata": {},
   "source": [
    "**Ques57.** What is cross-validation in machine learning?\n",
    "\n",
    "Cross-validation is a technique used in machine learning to assess the performance and generalization capability of a model. It involves splitting the available data into multiple subsets, or folds, to train and evaluate the model iteratively. Each fold is used as a validation set while the remaining folds are used as the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0977d0fd",
   "metadata": {},
   "source": [
    "**Ques58.** Why is cross-validation important?\n",
    "\n",
    "Cross-validation is important in machine learning for the following reasons:\n",
    "\n",
    "1. Performance Estimation: Cross-validation provides a more reliable estimate of the model's performance compared to a single train-test split. By evaluating the model on multiple folds, it helps to mitigate the impact of data variability and provides a more robust estimate of how well the model is likely to perform on unseen data.\n",
    "\n",
    "2. Model Selection: Cross-validation is useful for comparing and selecting between different models or hyperparameter settings. By evaluating each model on multiple folds, it allows for a fair comparison of performance and helps in selecting the best-performing model.\n",
    "\n",
    "3. Avoiding Overfitting: Cross-validation helps in assessing whether a model is overfitting or underfitting the data. If a model performs significantly better on the training data compared to the validation data, it indicates overfitting. Cross-validation helps to identify such instances and guides model adjustments or feature selection to improve generalization.\n",
    "\n",
    "4. Data Utilization: Cross-validation allows for maximum utilization of available data. In k-fold cross-validation, each data point is used for both training and validation, ensuring that all instances contribute to the overall model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d07b6a",
   "metadata": {},
   "source": [
    "**Ques59.** Explain the difference between k-fold cross-validation and stratified k-fold cross-validation.\n",
    "\n",
    "1. K-fold Cross-Validation:\n",
    "In k-fold cross-validation, the available data is divided into k equal-sized folds. The model is trained and evaluated k times, with each fold serving as the validation set once and the remaining k-1 folds used as the training set. The performance metric is computed for each iteration, and the average performance across all iterations is considered as the model's performance estimate.\n",
    "\n",
    "K-fold cross-validation is widely used when the data distribution is assumed to be uniform and there is no concern about class imbalance or unequal representation of different classes or categories in the data. It provides a robust estimate of the model's performance and helps in comparing different models or hyperparameter settings.\n",
    "\n",
    "2. Stratified K-fold Cross-Validation:\n",
    "Stratified k-fold cross-validation is an extension of k-fold cross-validation that takes into account the class or category distribution in the data. It ensures that each fold has a similar distribution of classes, preserving the class proportions observed in the overall dataset.\n",
    "\n",
    "Stratified k-fold cross-validation is particularly useful when dealing with imbalanced datasets where one or more classes are significantly underrepresented. By preserving the class proportions, it helps in obtaining more reliable and representative performance estimates for models, especially in scenarios where correct classification of minority classes is of high importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1095d0",
   "metadata": {},
   "source": [
    "**Ques60.** How do you interpret the cross-validation results?\n",
    "\n",
    "1. Performance Metrics: Evaluate the model's performance on each fold using appropriate evaluation metrics. Common metrics include accuracy, precision, recall, F1 score, and area under the ROC curve (AUC-ROC). Calculate the average and standard deviation of these metrics across all folds.\n",
    "\n",
    "2. Consistency: Check the consistency of the performance metrics across different folds. If the metrics show low variance or standard deviation across folds, it indicates that the model's performance is stable and consistent across different subsets of the data. This suggests a reliable and robust model.\n",
    "\n",
    "3. Bias-Variance Trade-off: Analyze the trade-off between bias and variance. If the model consistently performs well across all folds and the metrics are close to each other, it suggests a well-balanced model with low bias and low variance. Conversely, if the performance metrics vary significantly across folds, it may indicate high variance, overfitting, or issues with generalization.\n",
    "\n",
    "4. Comparison to Baseline: Compare the model's performance metrics against a baseline model or a benchmark. If the model consistently outperforms the baseline across all folds, it indicates the model's effectiveness. However, if the model performs similarly or worse than the baseline, it may indicate that the model needs improvement or that the dataset is challenging.\n",
    "\n",
    "5. Identify Limitations: Identify any patterns or trends in the performance metrics across folds. For example, if the model consistently performs well on certain subsets of the data (e.g., specific classes or instances), it may suggest that the model is biased or overfitting to those subsets. Understanding these limitations can guide further model refinement or data collection strategies.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
